name: quoridor-engine
version: "0.1"
author: James Harmon <JamesMHarmon@gmail.com>
about: AlphaZero in Rust
settings:
    - ArgRequiredElseHelp

subcommands:
    - init:
        about: Initializes a new run of a game.
        args:
            - game:
                required: true
                long: game
                short: g
                takes_value: true
                value_name: GAME NAME
                help: Name of the game to run.
            - run:
                required: true
                long: run
                short: r
                takes_value: true
                value_name: RUN NAME
                help: Name of the run, cannot contain '_'.
            - number_of_games_per_net:
                long: number_of_games_per_net
                takes_value: true
                help: "[Default: 32,000] The number of games to self play until a new net is generated. AZ used 5,000."
                value_name: SIZE
            - self_play_batch_size:
                long: self_play_batch_size
                takes_value: true
                help: "[Default: 256] The number of games to play in parallel during self play."
                value_name: SIZE
            - moving_window_size:
                long: moving_window_size
                takes_value: true
                help: "[Default: 500,000] How many games to go back and sample from when training a new net. AZ used 1,000,000."
                value_name: SIZE
            - max_moving_window_percentage:
                long: max_moving_window_percentage
                takes_value: true
                help: "[Default: 0.5] Applies a maximum amount of games within the window in relation to all of the games played. This allows for the window to have a smaller size in the beginning to expedite learning. A value of 1 would have no affect."
                value_name: PERCENTAGE
            - position_sample_percentage:
                long: position_sample_percentage
                takes_value: true
                help: "[Default: 0.0512] What percentage of positions to be sampled from each game. AZ - 2,048 * 1000 / 500,000 / 80 = 0.0512 \"mini-batch size of 2,048. Each mini-batch of data is sampled uniformly at random from all positions of the most recent 500,000 games of self-play. Neural network parameters are optimized by stochastic gradient descent with momentum and learning rate annealing, using the loss in equation (1). The learning rate is annealed according to the standard schedule in Extended Data Table 3. The momentum parameter is set to 0.9. The cross-entropy and MSE losses are weighted equally (this is reasonable because rewards are unit scaled, and the L2 regularization parameter is set to c=10. The optimization process produces a new checkpoint every 1,000 training steps. This checkpoint is evaluated by the evaluator and it may be used for generating the next batch of self-play games, as we explain next.\""
                value_name: PERCENTAGE
            - train_ratio:
                long: train_ratio
                takes_value: true
                help: "[Default: 0.9] The ratio of training to test data when training a new net."
                value_name: RATIO
            - train_batch_size:
                long: train_batch_size
                takes_value: true
                help: "[Default: 512] The batch size of sample games to provide a net during training."
                value_name: SIZE
            - epochs:
                long: epochs
                takes_value: true
                help: "[Default: 1] The number of epochs to use when training a net."
                value_name: EPOCHS
            - learning_rate:
                long: learning_rate
                takes_value: true
                help: "[Default: 0.1] The learning rate to use when training a net. AZ used the following schedule 0: 2e-1, 100e3: 2e-2, 300e3: 2e-3, 500e3: 2e-4."
                value_name: RATE
            - policy_loss_weight:
                long: policy_loss_weight
                takes_value: true
                help: "[Default: 1.0] The weight of the policy loss to apply compared to the value loss."
                value_name: LOSS
            - value_loss_weight:
                long: value_loss_weight
                takes_value: true
                help: "[Default: 0.5] The weight of the value loss to apply compared to the policy loss."
                value_name: LOSS
            - temperature:
                long: temperature
                takes_value: true
                help: "[Default: 1.0] The temperature to use during self play games before the temperature_max_actions threshold has been met. The higher the temp, the more exploration."
                value_name: TEMP
            - temperature_max_actions:
                long: temperature_max_actions
                takes_value: true
                help: "[Default: 30] The number of actions in the game to be made before changing the temperature to temperature_post_max_actions."
                value_name: NUM
            - temperature_post_max_actions:
                long: temperature_post_max_actions
                takes_value: true
                help: "[Default: 0.45] The temperature to use during self play games after the temperature_max_actions threshold has been met. The higher the temp, the more exploration."
                value_name: TEMP
            - visits:
                long: visits
                takes_value: true
                help: "[Default: 800] The number of visits of the root node as part of the mcts before deciding on a move to take. AZ used 800."
                value_name: VISITS
            - fpu:
                long: fpu
                takes_value: true
                help: "[Default: 0.0] First Player Urgency. This value determine the value of an action that hasn't yet been evaluated. This is used during the caclulation of cpuct"
                value_name: NUM
            - fpu_root:
                long: fpu_root
                takes_value: true
                help: "[Default: 1.0] First Player Urgency. This value determine the value of an action that hasn't yet been evaluated. This is used during the caclulation of cpuct"
                value_name: NUM
            - cpuct_base:
                long: cpuct_base
                takes_value: true
                help: "[Default: 19,652] Adds a scaling factor for cpuct, the more visits that a node has, the more that exploration over exploitation will be favored. AZ used 19,652."
                value_name: NUM
            - cpuct_init:
                long: cpuct_init
                takes_value: true
                help: "[Default: 1.25] The cpuct value within the MCTS algorithm. A higher cpuct encourages exploration over exploitation. AZ used 1.25."
                value_name: NUM
            - cpuct_root_scaling:
                long: cpuct_root_scaling
                takes_value: true
                help: "[Default: 2.0] The cpuct value within the MCTS algorithm. Scales cpu by the provided value when evaluating the children of the root node."
                value_name: NUM
            - alpha:
                long: alpha
                takes_value: true
                help: "[Default: 0.3] The alpha value within the MCTS algorithm specific to Dirichlet noise. AZ used 0.3."
                value_name: NUM
            - epsilon:
                long: epsilon
                takes_value: true
                help: "[Default: 0.25] The epsilon value within the MCTS algorithm specific to Dirichlet noise. AZ used 0.25."
                value_name: NUM
            - number_of_filters:
                long: number_of_filters
                takes_value: true
                help: "[Default: 64] The number of number of filters to use within a convolution layer as part of the net."
                value_name: NUM
            - number_of_residual_blocks:
                long: number_of_residual_blocks
                takes_value: true
                help: "[Default: 5] The number of residual blocks to use as part of the net."
                value_name: NUM
    - run:
        about: Continues on an existing run.
        args:
            - game:
                required: true
                short: g
                long: game
                takes_value: true
                value_name: GAME NAME
                help: Name of the game to run.
            - run:
                required: true
                short: r
                long: run
                takes_value: true
                value_name: RUN NAME
                help: Name of the run, cannot contain '_'.
    - evaluate:
        about: Pits two models against each other.
        args:
            - game:
                required: true
                short: g
                long: game
                takes_value: true
                value_name: GAME NAME
                help: Name of the game to run.
            - run:
                required: true
                short: r
                long: run
                takes_value: true
                value_name: RUN NAME
                help: Name of the run, cannot contain '_'.
            - model_1:
                short: m1
                long: model_1
                takes_value: true
                value_name: MODEL 1
                help: "[Default: Second Most Latest Model] The number of the model that is the p1 challenger."
            - model_2:
                short: m2
                long: model_2
                takes_value: true
                value_name: MODEL 2
                help: "[Default: Latest Model] The number of the model that is the p2 challenger."
            - num_games:
                short: "n"
                long: num_games
                takes_value: true
                value_name: NUM GAMES
                help: "[Default: 1,000] The number of the model that is the p2 challenger."
